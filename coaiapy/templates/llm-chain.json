{
  "name": "llm-chain",
  "version": "1.0",
  "description": "Sequential LLM calls with context passing",
  "author": "CoaiaPy Built-in", 
  "variables": [
    {
      "name": "chain_name",
      "type": "string",
      "required": true,
      "description": "Name for the LLM chain"
    },
    {
      "name": "model_name",
      "type": "string",
      "required": false,
      "default": "gpt-4",
      "choices": ["gpt-4", "gpt-3.5-turbo", "claude-3", "llama-2"],
      "description": "LLM model to use"
    },
    {
      "name": "initial_prompt",
      "type": "string",
      "required": true,
      "description": "Initial prompt for the chain"
    },
    {
      "name": "user_id",
      "type": "string",
      "required": false,
      "description": "User running the chain"
    },
    {
      "name": "temperature",
      "type": "number",
      "required": false,
      "default": 0.7,
      "description": "Temperature setting for model"
    },
    {
      "name": "enable_judge_evaluation",
      "type": "boolean",
      "required": false,
      "default": false,
      "description": "Enable automatic LLM judge evaluation of generated content"
    },
    {
      "name": "judge_model",
      "type": "string",
      "required": false,
      "default": "gpt-4",
      "choices": ["gpt-4", "gpt-3.5-turbo", "claude-3", "gpt-4-turbo"],
      "description": "LLM model to use as judge (when evaluation enabled)"
    },
    {
      "name": "evaluation_criteria",
      "type": "string",
      "required": false,
      "default": "helpfulness",
      "choices": ["helpfulness", "correctness", "clarity", "relevance"],
      "description": "Criteria for judge evaluation (when evaluation enabled)"
    }
  ],
  "steps": [
    {
      "name": "LLM Chain: {{chain_name}}",
      "observation_type": "SPAN",
      "description": "Sequential LLM processing chain using {{model_name}}",
      "variables": {
        "input": {"chain_name": "{{chain_name}}", "model": "{{model_name}}"},
        "output": {"status": "chain_started"}
      },
      "metadata": {
        "template": "llm-chain",
        "model": "{{model_name}}",
        "temperature": "{{temperature}}",
        "user_id": "{{user_id}}"
      }
    },
    {
      "name": "Initial Generation",
      "observation_type": "GENERATION",
      "description": "First LLM call in {{chain_name}}",
      "parent": "LLM Chain: {{chain_name}}",
      "variables": {
        "input": {"prompt": "{{initial_prompt}}", "model": "{{model_name}}"},
        "output": {"response": "Initial generation complete", "tokens_used": 150}
      },
      "metadata": {
        "step": "initial",
        "model": "{{model_name}}",
        "temperature": "{{temperature}}",
        "usage": {"prompt_tokens": 50, "completion_tokens": 100, "total_tokens": 150}
      }
    },
    {
      "name": "Context Processing", 
      "observation_type": "EVENT",
      "description": "Processing context from previous generation",
      "parent": "LLM Chain: {{chain_name}}",
      "variables": {
        "input": {"action": "process_context", "previous_response": "from_initial_generation"},
        "output": {"processed_context": "Enhanced context for next call"}
      },
      "metadata": {
        "step": "context_processing",
        "context_length": 200
      }
    },
    {
      "name": "Follow-up Generation",
      "observation_type": "GENERATION", 
      "description": "Second LLM call with enhanced context",
      "parent": "LLM Chain: {{chain_name}}",
      "variables": {
        "input": {"prompt": "Enhanced prompt with context", "model": "{{model_name}}"},
        "output": {"response": "Follow-up generation complete", "tokens_used": 180}
      },
      "metadata": {
        "step": "followup",
        "model": "{{model_name}}",
        "temperature": "{{temperature}}",
        "usage": {"prompt_tokens": 80, "completion_tokens": 100, "total_tokens": 180}
      }
    },
    {
      "name": "Final Synthesis",
      "observation_type": "EVENT",
      "description": "Synthesizing results from LLM chain",
      "parent": "LLM Chain: {{chain_name}}",
      "variables": {
        "input": {"action": "synthesize", "responses": ["initial", "followup"]},
        "output": {"final_result": "Chain synthesis complete", "confidence": 0.95}
      },
      "metadata": {
        "step": "synthesis",
        "total_tokens": 330,
        "chain_complete": true
      }
    },
    {
      "name": "Judge Initial Generation",
      "observation_type": "GENERATION",
      "description": "{{judge_model}} evaluating initial generation for {{evaluation_criteria}}",
      "parent": "LLM Chain: {{chain_name}}",
      "conditional": "{{enable_judge_evaluation}}",
      "variables": {
        "input": {
          "content": "Initial generation response",
          "evaluation_prompt": "Evaluate the initial generation for {{evaluation_criteria}}. Rate 1-10 and provide reasoning.",
          "model": "{{judge_model}}"
        },
        "output": {"judge_score": 8.3, "judge_reasoning": "Strong {{evaluation_criteria}} with clear response structure"}
      },
      "metadata": {
        "step": "judge_initial",
        "model": "{{judge_model}}",
        "evaluation_criteria": "{{evaluation_criteria}}",
        "generation_step": "initial"
      }
    },
    {
      "name": "Judge Follow-up Generation",
      "observation_type": "GENERATION",
      "description": "{{judge_model}} evaluating follow-up generation for {{evaluation_criteria}}",
      "parent": "LLM Chain: {{chain_name}}",
      "conditional": "{{enable_judge_evaluation}}",
      "variables": {
        "input": {
          "content": "Follow-up generation response",
          "evaluation_prompt": "Evaluate the follow-up generation for {{evaluation_criteria}}. Rate 1-10 and provide reasoning.",
          "model": "{{judge_model}}"
        },
        "output": {"judge_score": 8.7, "judge_reasoning": "Excellent {{evaluation_criteria}} with contextual enhancement"}
      },
      "metadata": {
        "step": "judge_followup",
        "model": "{{judge_model}}",
        "evaluation_criteria": "{{evaluation_criteria}}",
        "generation_step": "followup"
      }
    },
    {
      "name": "Chain Quality Summary",
      "observation_type": "EVENT",
      "description": "Summary of judge evaluations for the complete LLM chain",
      "parent": "LLM Chain: {{chain_name}}",
      "conditional": "{{enable_judge_evaluation}}",
      "variables": {
        "input": {"action": "summarize_chain_quality", "chain_name": "{{chain_name}}"},
        "output": {
          "overall_quality_score": 8.5,
          "initial_score": 8.3,
          "followup_score": 8.7,
          "quality_trend": "improving",
          "evaluation_complete": true
        }
      },
      "metadata": {
        "step": "chain_quality_summary",
        "judge_model": "{{judge_model}}",
        "evaluation_criteria": "{{evaluation_criteria}}",
        "chain_evaluation_complete": true
      }
    }
  ]
}